import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, accuracy_score


# Define the GAN architecture
def build_generator(input_dim, output_dim):
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_dim=input_dim),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.Dense(output_dim, activation='tanh')
    ])
    return model


def build_discriminator(input_dim):
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(512, activation='relu', input_dim=input_dim),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    return model


# Function to generate fake samples
def generate_fake_samples(generator, latent_dim, n_samples):
    x_input = np.random.randn(latent_dim * n_samples)
    x_input = x_input.reshape(n_samples, latent_dim)
    X_fake = generator.predict(x_input)
    y_fake = np.zeros((n_samples, 1))
    return X_fake, y_fake


if __name__ == '__main__':

    # Load your dataset
    df = pd.read_csv('../datasets/student-por.csv')

    df["Approved"] = df["G3"].apply(lambda x: 1 if x >= 10 else 0)
    df = df.drop(columns=['G3', 'G2'])
    # Define your features (X) and target (y)
    X = df.drop('target_column', axis=1)
    y = df['target_column']

    # Split into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

    # Preprocessing: Identify numerical and categorical features
    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns
    categorical_features = X.select_dtypes(include=['object', 'category']).columns

    # Define a ColumnTransformer for preprocessing
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numerical_features),
            ('cat', OneHotEncoder(), categorical_features)
        ])

    # Preprocess the training data
    X_train_preprocessed = preprocessor.fit_transform(X_train)

    # GAN hyperparameters
    latent_dim = 100
    input_dim = X_train_preprocessed.shape[1]
    epochs = 100
    batch_size = 64

    # Build and compile the discriminator
    discriminator = build_discriminator(input_dim)
    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Build the generator
    generator = build_generator(latent_dim, input_dim)

    # Build the GAN by combining generator and discriminator
    gan = tf.keras.Sequential([generator, discriminator])
    discriminator.trainable = False
    gan.compile(optimizer='adam', loss='binary_crossentropy')

    # Train the GAN
    for epoch in range(epochs):
        # Select a random batch of real samples
        idx = np.random.randint(0, X_train_preprocessed.shape[0], batch_size)
        X_real, y_real = X_train_preprocessed[idx], np.ones((batch_size, 1))

        # Generate fake samples
        X_fake, y_fake = generate_fake_samples(generator, latent_dim, batch_size)

        # Train the discriminator
        d_loss_real = discriminator.train_on_batch(X_real, y_real)
        d_loss_fake = discriminator.train_on_batch(X_fake, y_fake)

        # Prepare noise input for the generator
        x_gan = np.random.randn(latent_dim * batch_size)
        x_gan = x_gan.reshape(batch_size, latent_dim)
        y_gan = np.ones((batch_size, 1))

        # Train the generator
        g_loss = gan.train_on_batch(x_gan, y_gan)

        # Print progress every 1000 epochs
        if epoch % 1000 == 0:
            print(
                f"Epoch {epoch + 1}/{epochs}, D Loss Real: {d_loss_real}, D Loss Fake: {d_loss_fake}, G Loss: {g_loss}")

    # Generate synthetic samples using the trained GAN
    X_synthetic, _ = generate_fake_samples(generator, latent_dim, X_train_preprocessed.shape[0])

    # Combine original and synthetic data
    X_train_combined = np.vstack((X_train_preprocessed, X_synthetic))
    y_train_combined = np.hstack(
        (y_train, np.ones_like(y_train)))  # Assuming synthetic data belongs to the minority class

    # Train the model with the combined dataset
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', GradientBoostingClassifier(random_state=42))
    ])

    pipeline.fit(X_train_combined, y_train_combined)
    y_pred_synthetic = pipeline.predict(X_test)
    print("Results with synthetic data generated by GAN:")
    print(classification_report(y_test, y_pred_synthetic))
    print("Accuracy:", accuracy_score(y_test, y_pred_synthetic))
